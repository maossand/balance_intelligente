{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52T8psoGCj4L"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtdwJXACl1NB"
      },
      "outputs": [],
      "source": [
        "# Essential Libraries\n",
        "import os  # File management\n",
        "import shutil  # Copying/moving files\n",
        "import zipfile  # Handling compressed files\n",
        "\n",
        "# Data Processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg  # displaying images\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "# Serialization & Model Saving\n",
        "import joblib  # Loading scalers\n",
        "import pickle  # Loading Python objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFM0k4kOCwXp"
      },
      "source": [
        "# Import Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-28T02:09:24.912489Z",
          "iopub.status.busy": "2025-02-28T02:09:24.912283Z",
          "iopub.status.idle": "2025-02-28T02:09:24.948661Z",
          "shell.execute_reply": "2025-02-28T02:09:24.947756Z",
          "shell.execute_reply.started": "2025-02-28T02:09:24.912471Z"
        },
        "id": "1HR1AgUxl1NC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Load the dataset\n",
        "df = pd.read_csv(\"../../csv/20250301_df_19_clean_filtered.csv\")\n",
        "\n",
        "#  Generate 'file_path' based on 'subset' column\n",
        "df[\"file_path\"] = df.apply(\n",
        "    lambda row: f\"/content/augmented_images_test/{row['variety']}/{row['file_name']}\"\n",
        "    if row[\"subset\"] == \"test\"\n",
        "    else f\"/content/augmented_images_train/{row['variety']}/{row['file_name']}\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(columns=[\"simp_amount\", \"rank\"], inplace=True)\n",
        "\n",
        "#  Rename & Reorder columns\n",
        "df.rename(columns={\"adjusted_weight\": \"weight\"}, inplace=True)\n",
        "df = df[[\"file_path\", \"label\", \"packed\", \"crowd\", \"amount\", \"weight\", \"subset\"]]\n",
        "\n",
        "#  Create Test Set\n",
        "df_test = df[df[\"subset\"] == \"test\"].drop(columns=[\"subset\"])\n",
        "\n",
        "#  Modify file paths for test images\n",
        "df_test[\"file_path\"] = df_test[\"file_path\"].str.replace(\"/content\", \"../test\", regex=False)\n",
        "\n",
        "#  Display first 2 rows\n",
        "df_test.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create dictionary to identify fruits by their label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of fruit/vegetable varieties (same order as for the segmentation model (YOLO))\n",
        "fruits_only = [\n",
        "    \"carrot_carrot\", \"apple_red_delicious\", \"tomato_pink\", \"cucumber_long\", \"banana_yellow\",\n",
        "    \"apple_granny\", \"apple_fuji\", \"pepper_sweet_red\", \"orange_orange\", \"onion_white\",\n",
        "    \"apple_ligol\", \"lime_lime\", \"avocado_hass\", \"apple_golden\", \"kiwi_kiwi\",\n",
        "    \"tomato_cherry_red\", \"pepper_sweet_green\", \"pepper_sweet_yellow\", \"lemon_yellow\"\n",
        "]\n",
        "\n",
        "# Create dictionary with matching indices\n",
        "varieties_dict = {idx: variety for idx, variety in enumerate(fruits_only)}\n",
        "\n",
        "print(varieties_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Mean Weight and Scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5ZkGHd8RUfi"
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "mean_weight_path = \"../model/weight_scaler/mean_weight.pkl\"\n",
        "scaler_path = \"../model/weight_scaler/weight_scaler_sample1.pkl\"\n",
        "amount_scaler_path = '../model/weight_scaler/amount_scaler_sample1.pkl'\n",
        "\n",
        "\n",
        "# Load the mean weight\n",
        "with open(mean_weight_path, \"rb\") as f:\n",
        "    mean_weight = pickle.load(f)\n",
        "\n",
        "# Load the weight scaler (input) and the count scaler (output)\n",
        "weight_scaler = joblib.load(scaler_path)\n",
        "count_scaler = joblib.load(amount_scaler_path)\n",
        "\n",
        "# Convert to NumPy array & reshape, convert to DataFrame for consistency before scaling\n",
        "test_weights = df_test[\"weight\"].values.reshape(-1, 1)  \n",
        "test_weights_df = pd.DataFrame(test_weights, columns=[\"weight\"])\n",
        "\n",
        "# Scale the weights using the loaded scaler\n",
        "test_weights_scaled = weight_scaler.transform(test_weights_df)\n",
        "\n",
        "print(\" Test weights successfully loaded and scaled!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEAqN36of3Cm"
      },
      "outputs": [],
      "source": [
        "#  Load test dataset features\n",
        "test_image_paths = df_test[\"file_path\"].values\n",
        "test_weights = df_test[\"weight\"].values\n",
        "test_labels = df_test[\"label\"].values\n",
        "test_amounts = df_test[\"amount\"].values\n",
        "test_packed = df_test[\"packed\"].values\n",
        "test_crowd = df_test[\"crowd\"].values\n",
        "\n",
        "#  Function to load and preprocess a single image\n",
        "def load_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)  # Decode JPEG\n",
        "    image = tf.image.resize(image, (224, 224))  # Resize for model input\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0,1]\n",
        "    return image\n",
        "\n",
        "#  Load images using `map()`\n",
        "test_images = np.array([load_image(img).numpy() for img in test_image_paths])\n",
        "\n",
        "print(f\"Loaded {len(test_images)} test images successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the trained model with custom Huber loss\n",
        "model_path = \"../model/models/best_model_EfficientNet.keras\"\n",
        "model = load_model(model_path, custom_objects={\"Huber\": Huber})\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PChnzYxtezPu"
      },
      "source": [
        "# I. Prediction (including the weight feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGWO3_PqRUUu"
      },
      "outputs": [],
      "source": [
        "# Get model predictions\n",
        "\n",
        "predictions = model.predict([test_images, test_weights_scaled])\n",
        "batch_size = 32  # Adjust based on available RAM\n",
        "num_batches = len(test_images) // batch_size + 1  # Total batches\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for i in range(num_batches):\n",
        "    batch_images = test_images[i * batch_size: (i + 1) * batch_size]\n",
        "    batch_weights = test_weights_scaled[i * batch_size: (i + 1) * batch_size]\n",
        "    \n",
        "    batch_preds = model.predict([batch_images, batch_weights], verbose=0)\n",
        "    predictions.append(batch_preds)\n",
        "\n",
        "# Combine batch predictions\n",
        "predictions = [np.concatenate([batch[i] for batch in predictions], axis=0) for i in range(len(predictions[0]))]\n",
        "\n",
        "print(\"Testing completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract predictions for each output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVEqf0wvRURQ"
      },
      "outputs": [],
      "source": [
        "# Predictions\n",
        "\n",
        "fruit_class_preds = np.argmax(predictions[0], axis=1)  # Convert softmax to class index\n",
        "fruit_count_preds = predictions[1].flatten()  # Regression output\n",
        "bagged_preds = (predictions[2].flatten() > 0.5).astype(int)  # Convert sigmoid output to binary\n",
        "crowded_preds = (predictions[3].flatten() > 0.5).astype(int)  # Convert sigmoid output to binary\n",
        "\n",
        "# Evaluation metrics\n",
        "\n",
        "fruit_class_accuracy = accuracy_score(test_labels, fruit_class_preds)\n",
        "bagged_accuracy = accuracy_score(test_packed, bagged_preds)\n",
        "crowded_accuracy = accuracy_score(test_crowd, crowded_preds)\n",
        "huber_loss_mean = tf.losses.huber(test_amounts, fruit_count_preds, delta=1.0).numpy()\n",
        "\n",
        "print(\"Fruit Classification Accuracy:\", round(fruit_class_accuracy, 2))\n",
        "print(\"Huber Loss:\", round(huber_loss_mean,2))\n",
        "print(\"Bagged Accuracy:\", round(bagged_accuracy,2))\n",
        "print(\"Crowded Accuracy:\", round(crowded_accuracy,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create metrics dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale the count results back to their original form. \n",
        "\n",
        "fruit_count_preds = np.array(fruit_count_preds).reshape(-1, 1)\n",
        "fruit_count_preds_original = count_scaler.inverse_transform(fruit_count_preds).flatten()\n",
        "\n",
        "# Create a dataframe for comparison\n",
        "results_df = pd.DataFrame({\n",
        "    \"actual_fruit_class\": test_labels,\n",
        "    \"predicted_fruit_class\": fruit_class_preds,\n",
        "    \"actual_fruit_count\": test_amounts,\n",
        "    \"predicted_fruit_count\": fruit_count_preds_original,\n",
        "    \"actual_bagged\": test_packed,\n",
        "    \"predicted_bagged\": bagged_preds,\n",
        "    \"actual_crowded\": test_crowd,\n",
        "    \"predicted_crowded\": crowded_preds\n",
        "})\n",
        "\n",
        "# Add the error counts and fruit names\n",
        "\n",
        "results_df['count_error'] = results_df['actual_fruit_count'] - results_df['predicted_fruit_count']\n",
        "results_df['predicted_fruit_name'] = results_df['predicted_fruit_class'].map(varieties_dict)\n",
        "results_df['actual_fruit_name'] = results_df['actual_fruit_class'].map(varieties_dict)\n",
        "\n",
        "# Save results to a CSV for further analysis\n",
        "results_df.to_csv(\"test_results_withWeight.csv\", index=False)\n",
        "\n",
        "print(\"Results saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load metrics dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sxl0rGSAZSjB"
      },
      "outputs": [],
      "source": [
        "# Load the uploaded test results CSV file\n",
        "file_path = \"test_results_withWeight.csv\"\n",
        "results_df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "results_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualisations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix for Fruit Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract unique fruit names sorted for consistent label mapping\n",
        "fruit_names = sorted(results_df[\"actual_fruit_name\"].unique())\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(results_df[\"actual_fruit_name\"], results_df[\"predicted_fruit_name\"], labels=fruit_names)\n",
        "\n",
        "# Plot heatmap with fruit name labels\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"BuGn\", xticklabels=fruit_names, yticklabels=fruit_names)\n",
        "\n",
        "plt.xlabel(\"Predicted Fruit\")\n",
        "plt.ylabel(\"Actual Fruit\")\n",
        "plt.title(\"Confusion Matrix for Fruit Classification (with weight of fruits)\")\n",
        "plt.xticks(rotation=90)  # Rotate labels for better readability\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distribution of Errors in Fruit Count Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of Errors in Fruit Count Predictions\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(results_df[\"count_error\"], bins=30, kde=True, color=\"green\")\n",
        "plt.axvline(0, color=\"red\", linestyle=\"dashed\", label=\"Perfect Prediction (Error = 0)\")\n",
        "plt.xlabel(\"Prediction Error (Actual - Predicted)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Errors in Fruit Count Predictions (with weight of fruits)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regression Performance for Fruit Count (Scatter Plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regression Performance for Fruit Count (Scatter Plot)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(results_df[\"actual_fruit_count\"], results_df[\"predicted_fruit_count\"], alpha=0.5, color='green')\n",
        "plt.xlabel(\"Actual Fruit Count\")\n",
        "plt.ylabel(\"Predicted Fruit Count\")\n",
        "plt.title(\"Regression Performance for Fruit Count (with weight of fruits)\")\n",
        "plt.axline((0, 0), slope=1, color=\"red\", linestyle=\"dashed\", label=\"Perfect Prediction Line\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Binary Classification Performance: ROC Curve for Bagged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Binary Classification Performance (ROC Curve for Bagged & Crowded)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Bagged ROC Curve\n",
        "plt.subplot(1, 2, 1)\n",
        "fpr_bagged, tpr_bagged, _ = roc_curve(results_df[\"actual_bagged\"], results_df[\"predicted_bagged\"])\n",
        "roc_auc_bagged = auc(fpr_bagged, tpr_bagged)\n",
        "plt.plot(fpr_bagged, tpr_bagged, label=f\"AUC = {roc_auc_bagged:.2f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"dashed\", color=\"red\")  # Random chance line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Bagged Prediction\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Binary Classification Performance: ROC Curve for Crowded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crowded ROC Curve\n",
        "plt.subplot(1, 2, 2)\n",
        "fpr_crowded, tpr_crowded, _ = roc_curve(results_df[\"actual_crowded\"], results_df[\"predicted_crowded\"])\n",
        "roc_auc_crowded = auc(fpr_crowded, tpr_crowded)\n",
        "plt.plot(fpr_crowded, tpr_crowded, label=f\"AUC = {roc_auc_crowded:.2f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"dashed\", color=\"red\")  # Random chance line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Crowded Prediction\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# II. Predictions (image only)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set weight to 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a placeholder weight array filled with the mean weight\n",
        "num_samples = len(test_images)  # Match the number of test samples\n",
        "placeholder_weights = np.full((num_samples, 1), mean_weight)  # Shape (num_samples, 1)\n",
        "\n",
        "# Scale the placeholder weights\n",
        "placeholder_weights_scaled = weight_scaler.transform(placeholder_weights)\n",
        "\n",
        "batch_size = 32  \n",
        "predictions_weight0 = []  # Store batch predictions\n",
        "\n",
        "# Process test images in batches\n",
        "for i in range(0, len(test_images), batch_size):\n",
        "    batch_images = test_images[i: i + batch_size]\n",
        "    batch_weights = placeholder_weights_scaled[i: i + batch_size]  # Using placeholder weights\n",
        "\n",
        "    batch_preds = model.predict([batch_images, batch_weights], verbose=0)\n",
        "    predictions_weight0.append(batch_preds)\n",
        "\n",
        "# Combine batch predictions\n",
        "predictions_weight0 = [np.concatenate([batch[i] for batch in predictions_weight0], axis=0) for i in range(len(predictions_weight0[0]))]\n",
        "\n",
        "print(\"Testing completed successfully without requiring weight input!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract predictions for each output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract predictions for each output\n",
        "fruit_class_preds = np.argmax(predictions_weight0[0], axis=1)  # Convert softmax to class index\n",
        "fruit_count_preds = predictions_weight0[1].flatten()  # Regression output\n",
        "bagged_preds = (predictions_weight0[2].flatten() > 0.5).astype(int)  # Convert sigmoid output to binary\n",
        "crowded_preds = (predictions_weight0[3].flatten() > 0.5).astype(int)  # Convert sigmoid output to binary\n",
        "\n",
        "# Evaluation metrics\n",
        "\n",
        "fruit_class_accuracy = accuracy_score(test_labels, fruit_class_preds)\n",
        "bagged_accuracy = accuracy_score(test_packed, bagged_preds)\n",
        "crowded_accuracy = accuracy_score(test_crowd, crowded_preds)\n",
        "huber_loss_mean = tf.losses.huber(test_amounts, fruit_count_preds, delta=1.0).numpy()\n",
        "\n",
        "print(\"Fruit Classification Accuracy:\", round(fruit_class_accuracy, 2))\n",
        "print(\"Huber Loss:\", round(huber_loss_mean,2))\n",
        "print(\"Bagged Accuracy:\", round(bagged_accuracy,2))\n",
        "print(\"Crowded Accuracy:\", round(crowded_accuracy,2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create metrics dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale the count results back to their original form. \n",
        "\n",
        "fruit_count_preds = np.array(fruit_count_preds).reshape(-1, 1)\n",
        "fruit_count_preds_original = count_scaler.inverse_transform(fruit_count_preds).flatten()\n",
        "\n",
        "# Create a dataframe for comparison\n",
        "results_df_weight0 = pd.DataFrame({\n",
        "    \"actual_fruit_class\": test_labels,  # True labels from test set\n",
        "    \"predicted_fruit_class\": fruit_class_preds,\n",
        "    \"actual_fruit_count\": test_amounts,\n",
        "    \"predicted_fruit_count\": fruit_count_preds,\n",
        "    \"actual_bagged\": test_packed,\n",
        "    \"predicted_bagged\": bagged_preds,\n",
        "    \"actual_crowded\": test_crowd,\n",
        "    \"predicted_crowded\": crowded_preds\n",
        "})\n",
        "\n",
        "# Add the error counts and fruit names\n",
        "\n",
        "results_df_weight0['count_error'] = results_df_weight0['actual_fruit_count'] - results_df_weight0['predicted_fruit_count']\n",
        "results_df_weight0['predicted_fruit_name'] = results_df_weight0['predicted_fruit_class'].map(varieties_dict)\n",
        "results_df_weight0['actual_fruit_name'] = results_df_weight0['actual_fruit_class'].map(varieties_dict)\n",
        "\n",
        "# Save results to a CSV for further analysis\n",
        "results_df_weight0.to_csv(\"test_results_weight0.csv\", index=False)\n",
        "\n",
        "print(\"Results saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load metric dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the uploaded test results CSV file\n",
        "file_path = \"test_results_weight0.csv\"\n",
        "results_df_0 = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "results_df_0.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualisations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix for Fruit Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract unique fruit names sorted for consistent label mapping\n",
        "fruit_names = sorted(results_df_0[\"actual_fruit_name\"].unique())\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(results_df_0[\"actual_fruit_name\"], results_df_0[\"predicted_fruit_name\"], labels=fruit_names)\n",
        "\n",
        "# Plot heatmap with fruit name labels\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"BuGn\", xticklabels=fruit_names, yticklabels=fruit_names)\n",
        "\n",
        "plt.xlabel(\"Predicted Fruit\")\n",
        "plt.ylabel(\"Actual Fruit\")\n",
        "plt.title(\"Confusion Matrix for Fruit Classification (only image as input)\")\n",
        "plt.xticks(rotation=90)  # Rotate labels for better readability\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distribution of Errors in Fruit Count Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of Errors in Fruit Count Predictions\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(results_df_0[\"count_error\"], bins=30, kde=True, color=\"green\")\n",
        "plt.axvline(0, color=\"red\", linestyle=\"dashed\", label=\"Perfect Prediction (Error = 0)\")\n",
        "plt.xlabel(\"Prediction Error (Actual - Predicted)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Errors in Fruit Count Predictions (only image as input)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regression Performance for Fruit Count (Scatter Plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regression Performance for Fruit Count (Scatter Plot)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(results_df_0[\"actual_fruit_count\"], results_df_0[\"predicted_fruit_count\"], alpha=0.5, color='green')\n",
        "plt.xlabel(\"Actual Fruit Count\")\n",
        "plt.ylabel(\"Predicted Fruit Count\")\n",
        "plt.title(\"Regression Performance for Fruit Count (only image as input)\")\n",
        "plt.axline((0, 0), slope=1, color=\"red\", linestyle=\"dashed\", label=\"Perfect Prediction Line\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Binary Classification Performance: ROC Curve for Bagged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Binary Classification Performance (ROC Curve for Bagged & Crowded)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Bagged ROC Curve\n",
        "plt.subplot(1, 2, 1)\n",
        "fpr_bagged, tpr_bagged, _ = roc_curve(results_df_0[\"actual_bagged\"], results_df_0[\"predicted_bagged\"])\n",
        "roc_auc_bagged = auc(fpr_bagged, tpr_bagged)\n",
        "plt.plot(fpr_bagged, tpr_bagged, label=f\"AUC = {roc_auc_bagged:.2f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"dashed\", color=\"red\")  # Random chance line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Bagged Prediction (only image as input)\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Binary Classification Performance: ROC Curve for Crowded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crowded ROC Curve\n",
        "plt.subplot(1, 2, 2)\n",
        "fpr_crowded, tpr_crowded, _ = roc_curve(results_df_0[\"actual_crowded\"], results_df_0[\"predicted_crowded\"])\n",
        "roc_auc_crowded = auc(fpr_crowded, tpr_crowded)\n",
        "plt.plot(fpr_crowded, tpr_crowded, label=f\"AUC = {roc_auc_crowded:.2f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"dashed\", color=\"red\")  # Random chance line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Crowded Prediction (only image as input)\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6757027,
          "sourceId": 10875094,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6757122,
          "sourceId": 10875253,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Artefact",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
