{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 10875094,
          "sourceType": "datasetVersion",
          "datasetId": 6757027
        },
        {
          "sourceId": 10875253,
          "sourceType": "datasetVersion",
          "datasetId": 6757122
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "52T8psoGCj4L"
      }
    },
    {
      "source": [
        "# Essential Libraries\n",
        "import os  # File management\n",
        "import zipfile  # Handling compressed files\n",
        "from datetime import datetime  # Timestamping\n",
        "\n",
        "# Data Processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Deep Learning (TensorFlow/Keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
        "\n",
        "# Serialisation\n",
        "import pickle # Saving/loading Python objects\n",
        "import joblib\n",
        "\n",
        "# Set random seed for reproductibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "metadata": {
        "id": "BtdwJXACl1NB"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "7OaydSe5ChIt"
      }
    },
    {
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1Q_baLzDl1NC"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Images"
      ],
      "metadata": {
        "id": "-YRNjGWjCdNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = '/content/drive/MyDrive/packed_fruits/augmented_images_train.zip'\n",
        "\n",
        "# Directory to extract the zip contents temporarily\n",
        "extract_to_temp_dir = '/content/augmented_images_train/'\n",
        "\n",
        "# Create the temporary directory if it doesn't exist\n",
        "os.makedirs(extract_to_temp_dir, exist_ok=True)\n",
        "\n",
        "# Unzipping the file to the temporary directory\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_temp_dir)\n",
        "\n",
        "print(f\"Files extracted temporarily to: {extract_to_temp_dir}\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-28T02:08:57.718407Z",
          "iopub.execute_input": "2025-02-28T02:08:57.718705Z",
          "iopub.status.idle": "2025-02-28T02:09:24.911266Z",
          "shell.execute_reply.started": "2025-02-28T02:08:57.71868Z",
          "shell.execute_reply": "2025-02-28T02:09:24.91001Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "wO2VLIXKl1NC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataframe"
      ],
      "metadata": {
        "id": "yFM0k4kOCwXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/packed_fruits/20250301_df_19_clean_filtered.csv')\n",
        "\n",
        "df['file_path'] = df.apply(\n",
        "    lambda row: f\"/content/augmented_images_test/{row['variety']}/{row['file_name']}\"\n",
        "    if row['subset'] == 'test'\n",
        "    else f\"/content/augmented_images_train/{row['variety']}/{row['file_name']}\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df = df.drop(columns=['simp_amount', 'rank'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-28T02:09:24.912283Z",
          "iopub.execute_input": "2025-02-28T02:09:24.912489Z",
          "iopub.status.idle": "2025-02-28T02:09:24.948661Z",
          "shell.execute_reply.started": "2025-02-28T02:09:24.912471Z",
          "shell.execute_reply": "2025-02-28T02:09:24.947756Z"
        },
        "id": "1HR1AgUxl1NC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract train and validation sets"
      ],
      "metadata": {
        "id": "N6mljfRO7pPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['file_path', 'label', 'packed', 'crowd', 'amount', 'adjusted_weight', 'subset']]\n",
        "\n",
        "df = df.rename(columns={'adjusted_weight': 'weight'})\n",
        "\n",
        "df_train = df[df['subset'] == 'train'].drop('subset', axis=1)\n",
        "df_val = df[df['subset'] == 'validation'].drop('subset', axis=1)\n",
        "\n",
        "df_train_2 = df_train.copy()\n",
        "df_val_2 = df_val.copy()\n",
        "\n",
        "df_train.head(2)\n"
      ],
      "metadata": {
        "id": "y1FHcaV_G4gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select a Random Sample of the Data"
      ],
      "metadata": {
        "id": "3kAboFgdC7Z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before scaling we randomly slice the samples to reduce processing time\n",
        "# (here it is one because it corresponds to the full sample size)\n",
        "\n",
        "sample_fraction = 1\n",
        "\n",
        "df_train_2_sampled = df_train_2.sample(frac=sample_fraction)\n",
        "df_val_2_sampled = df_val_2.sample(frac=sample_fraction)\n",
        "\n",
        "train_df = df_train_2_sampled.copy()\n",
        "val_df = df_val_2_sampled.copy()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-28T02:09:25.068648Z",
          "iopub.execute_input": "2025-02-28T02:09:25.068923Z",
          "iopub.status.idle": "2025-02-28T02:09:25.085102Z",
          "shell.execute_reply.started": "2025-02-28T02:09:25.068902Z",
          "shell.execute_reply": "2025-02-28T02:09:25.084105Z"
        },
        "id": "qmLD1vc5l1ND"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling the Numerical Data"
      ],
      "metadata": {
        "id": "StrBymks5rf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the mean weight *before* scaling (for zero-centered standardization)\n",
        "# The mean weight will be used when testing the model *without* structured data (image only).\n",
        "# Since StandardScaler centers data, using mean_weight is equivalent to inputting 0.\n",
        "\n",
        "mean_weight = train_df[\"weight\"].mean()\n",
        "\n",
        "# Define save path for the mean weight\n",
        "save_path = \"/content/drive/MyDrive/packed_fruits/models_keras/scalars/\"\n",
        "mean_weight_filename = \"mean_weight.pkl\"\n",
        "full_save_path = os.path.join(save_path, mean_weight_filename)\n",
        "\n",
        "# Ensure directory exists before saving\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save mean weight for deployment\n",
        "with open(full_save_path, \"wb\") as f:\n",
        "    pickle.dump(mean_weight, f)\n",
        "\n",
        "print(f\"Mean weight saved at: {full_save_path}\")\n",
        "print(f\"Mean weight value: {mean_weight}\")\n",
        "\n",
        "# Initialize Standard Scalers\n",
        "weight_scaler = StandardScaler()\n",
        "amount_scaler = StandardScaler()\n",
        "\n",
        "# Fit scalers on training data and transform both train & validation sets\n",
        "train_weights_scaled = weight_scaler.fit_transform(train_df[[\"weight\"]])\n",
        "val_weights_scaled = weight_scaler.transform(val_df[[\"weight\"]])\n",
        "\n",
        "train_amounts_scaled = amount_scaler.fit_transform(train_df[[\"amount\"]])\n",
        "val_amounts_scaled = amount_scaler.transform(val_df[[\"amount\"]])\n",
        "\n",
        "# Save the scalers for later use (deployment)\n",
        "weight_scaler_filename = os.path.join(save_path, \"weight_scaler.pkl\")\n",
        "amount_scaler_filename = os.path.join(save_path, \"amount_scaler.pkl\")\n",
        "\n",
        "joblib.dump(weight_scaler, weight_scaler_filename)\n",
        "joblib.dump(amount_scaler, amount_scaler_filename)\n",
        "\n",
        "print(f\"Weight scaler saved at: {weight_scaler_filename}\")\n",
        "print(f\"Amount scaler saved at: {amount_scaler_filename}\")"
      ],
      "metadata": {
        "id": "p41txi-GdZZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create TensorFlow Dataset"
      ],
      "metadata": {
        "id": "6HwQOvwqCPh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TensorFlow expects float32 for numerical inputs (int64 might cause issues).\n",
        "\n",
        "# Convert file paths to NumPy arrays\n",
        "train_image_paths = train_df[\"file_path\"].values\n",
        "val_image_paths = val_df[\"file_path\"].values\n",
        "\n",
        "# Convert labels & numerical targets to correct data types\n",
        "train_labels = train_df[\"label\"].values.astype(\"int64\")\n",
        "val_labels = val_df[\"label\"].values.astype(\"int64\")\n",
        "\n",
        "# Regression target (fruit count)\n",
        "train_amounts = train_df[\"amount\"].values.astype(\"float32\")\n",
        "val_amounts = val_df[\"amount\"].values.astype(\"float32\")\n",
        "\n",
        "# Binary classification targets\n",
        "train_packed = train_df[\"packed\"].values.astype(\"float32\")\n",
        "val_packed = val_df[\"packed\"].values.astype(\"float32\")\n",
        "\n",
        "train_crowd = train_df[\"crowd\"].values.astype(\"float32\")\n",
        "val_crowd = val_df[\"crowd\"].values.astype(\"float32\")"
      ],
      "metadata": {
        "id": "mHhdLscaazf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Load and Preprocess Images\n"
      ],
      "metadata": {
        "id": "jL6rqqip6uVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess images\n",
        "def load_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    # image = tf.image.resize(image, (224, 224)) # in case images are not size 224x224\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0,1] range\n",
        "    return image\n",
        "\n",
        "# Function to load all inputs & outputs into TensorFlow dataset\n",
        "def load_data(image_path, weight, label, amount, packed, crowd):\n",
        "    image = load_image(image_path)\n",
        "    return {\n",
        "        \"image_input\": image,\n",
        "        \"structured_input\": tf.convert_to_tensor(weight, dtype=tf.float32)  # Structured input must be float32\n",
        "    }, {\n",
        "        \"fruit_class\": tf.convert_to_tensor(label, dtype=tf.int64),  # Classification labels must be int64 (for sparse_categorical_crossentropy)\n",
        "        \"fruit_count\": tf.convert_to_tensor(amount, dtype=tf.float32),  # Regression labels must be float32\n",
        "        \"bagged\": tf.convert_to_tensor(packed, dtype=tf.float32),  # Binary classification labels must be float32\n",
        "        \"crowded\": tf.convert_to_tensor(crowd, dtype=tf.float32)  # Binary classification labels must be float32\n",
        "    }\n",
        "\n",
        "# Create train dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_image_paths, train_weights_scaled.astype(np.float32), train_labels, train_amounts_scaled.astype(np.float32), train_packed, train_crowd)\n",
        ")\n",
        "train_dataset = train_dataset.map(load_data).shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Create validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (val_image_paths, val_weights_scaled.astype(np.float32), val_labels, val_amounts_scaled.astype(np.float32), val_packed, val_crowd)\n",
        ")\n",
        "val_dataset = val_dataset.map(load_data).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"TensorFlow Datasets Created Successfully!\")"
      ],
      "metadata": {
        "id": "5CkzK6LMazdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "DtatuLwJjKdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation Layer\n",
        "data_augmentation = keras.Sequential([\n",
        "    RandomFlip(\"horizontal\"),  # Randomly flip images\n",
        "    RandomRotation(0.2),       # Rotate by up to 20%\n",
        "    RandomZoom(0.1)            # Zoom in/out by 10%\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "# Image Input\n",
        "image_input = keras.Input(shape=(224, 224, 3), name=\"image_input\")\n",
        "x = data_augmentation(image_input)  # Augment images before passing to CNN\n",
        "\n",
        "# Pretrained CNN Backbone (EfficientNetB0)\n",
        "base_model = keras.applications.EfficientNetB0(include_top=False, input_tensor=image_input, weights='imagenet')\n",
        "\n",
        "# Feature Extraction\n",
        "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "x = layers.BatchNormalization()(x)  # Normalize activations\n",
        "x = layers.Dense(256, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.3)(x)  # Reduce overfitting\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "\n",
        "# Structured Input (Weight)\n",
        "structured_input = keras.Input(shape=(1,), name=\"structured_input\")\n",
        "y = layers.Dense(16, activation=\"relu\")(structured_input)\n",
        "y = layers.Dense(8, activation=\"relu\")(y)\n",
        "\n",
        "# Merge Image and Structured Features\n",
        "merged = layers.Concatenate()([x, y])\n",
        "\n",
        "# Output Layers\n",
        "outputs = {\n",
        "    \"fruit_class\": layers.Dense(19, activation=\"softmax\", name=\"fruit_class\")(merged),  # Multi-class classification\n",
        "    \"fruit_count\": layers.Dense(1, activation=\"linear\", name=\"fruit_count\")(merged),  # Regression\n",
        "    \"bagged\": layers.Dense(1, activation=\"sigmoid\", name=\"bagged\")(merged),  # Binary classification\n",
        "    \"crowded\": layers.Dense(1, activation=\"sigmoid\", name=\"crowded\")(merged),  # Binary classification\n",
        "}\n",
        "\n",
        "# Define Model\n",
        "model = keras.Model(inputs=[image_input, structured_input],\n",
        "                    outputs=outputs)\n",
        "\n",
        "# Compile Model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss={\n",
        "        \"fruit_class\": \"sparse_categorical_crossentropy\",  # Classification\n",
        "        \"fruit_count\": tf.keras.losses.Huber(delta=1.0),  # Huber Loss for regression\n",
        "        \"bagged\": \"binary_crossentropy\",  # Binary classification\n",
        "        \"crowded\": \"binary_crossentropy\",  # Binary classification\n",
        "    },\n",
        "    metrics={\n",
        "        \"fruit_class\": \"accuracy\",\n",
        "        \"fruit_count\": \"mae\",\n",
        "        \"bagged\": \"accuracy\",\n",
        "        \"crowded\": \"accuracy\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Model Summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "NqeBO9I8azam",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract timestamp for unique checkpoint filenames\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Define save path for model checkpoints\n",
        "save_path = \"/content/drive/MyDrive/packed_fruits/\"\n",
        "os.makedirs(save_path, exist_ok=True)  # Ensure directory exists\n",
        "\n",
        "# Callbacks for training optimization\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    factor=0.1,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1  # Show messages when learning rate is reduced\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    mode=\"min\",\n",
        "    verbose=1  # Show message when stopping early\n",
        ")\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=os.path.join(save_path, f\"best_model_{timestamp}.keras\"),  # Dynamic filename\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True,\n",
        "    verbose=1  # Print message when saving\n",
        ")\n",
        "\n",
        "# Store all callbacks in a list\n",
        "callbacks = [early_stopping, reduce_lr, model_checkpoint]"
      ],
      "metadata": {
        "id": "Z11vueVbos7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=70,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Model training complete!\")"
      ],
      "metadata": {
        "id": "EdqWvT1kazYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate timestamp for unique filenames\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Define save paths with timestamp\n",
        "model_path = f\"/content/drive/MyDrive/packed_fruits/model_{timestamp}.keras\"\n",
        "history_dir = \"/content/drive/MyDrive/packed_fruits/training_history/\"\n",
        "history_filename = f\"training_history_{timestamp}.pkl\"\n",
        "history_path = os.path.join(history_dir, history_filename)\n",
        "\n",
        "# Ensure the history save directory exists\n",
        "os.makedirs(history_dir, exist_ok=True)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "# Save training history\n",
        "with open(history_path, \"wb\") as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "print(f\"Training history saved to: {history_path}\")"
      ],
      "metadata": {
        "id": "CkyiD6uVqi7a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}